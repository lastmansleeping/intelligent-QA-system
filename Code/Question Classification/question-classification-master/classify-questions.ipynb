{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import gensim\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    res = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            label, question = line.split(\" \", 1)\n",
    "            res.append((label, question))\n",
    "    return res\n",
    "\n",
    "def average_vector2(dictionary, question):\n",
    "    cnt = 0\n",
    "    s = [0]*vector_dim\n",
    "    for w in question.split(\" \"):\n",
    "        w = w.lower()\n",
    "        cnt += 1\n",
    "        try:\n",
    "            # print word, word_vector[word]\n",
    "            s = map(operator.add, dictionary[w], s)\n",
    "        except KeyError:\n",
    "            cnt -= 1\n",
    "            # pass #Use random vector or skip?\n",
    "#             s = map(operator.add, dictionary.seeded_vector(random_generator(50)), s)\n",
    "    if cnt == 0:\n",
    "        return s\n",
    "    return [elem/float(cnt) for elem in s]\n",
    "\n",
    "def average_vector(dictionary, question):\n",
    "    splitted = question.split(\" \")\n",
    "    s = [0]*vector_dim\n",
    "    cnt = 2.0\n",
    "    try:\n",
    "        if (len(splitted) == 0):\n",
    "            return s\n",
    "        else:\n",
    "            s = map(operator.add, dictionary[splitted[0].lower()], s)\n",
    "            if (len(splitted) <= 1):\n",
    "                return s\n",
    "            s = map(operator.add, dictionary[splitted[1].lower()], s)\n",
    "            if (splitted[0].lower() == 'what' and splitted[1].lower() == 'is'):\n",
    "                return average_vector2(dictionary, question)\n",
    "#                 s = map(operator.add, dictionary[splitted[3].lower()], s)\n",
    "#                 cnt += 1.0\n",
    "            return [elem/cnt for elem in s]         \n",
    "    except KeyError:\n",
    "        return s \n",
    "    \n",
    "def compute_accuracy(predicted, original):\n",
    "    eq = [z[0] == z[1] for z in zip(predicted, original)]\n",
    "    return eq.count(True)/float(len(eq))\n",
    "\n",
    "def is_in_class(dictionary, questions, cls, train_lab, test_lab):\n",
    "    model = linear_model.LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
    "    tr_lab = [x.split(\":\")[0] for x in train_lab]\n",
    "    ts_lab = [x.split(\":\")[0] for x in test_lab]\n",
    "    model.fit(questions, np.array(tr_lab) == cls)\n",
    "    train_data_prediction = [model.predict(vec) for vec in questions]\n",
    "    test_data_prediction = [model.predict(average_vector(dictionary, line[1].lower())) for line in test_data]\n",
    "    print \"Train accuracy for class \" + cls + \": \" + str(compute_accuracy(train_data_prediction, np.array(tr_lab) == cls))\n",
    "    print \"Test accuracy for class \" + cls + \": \" + str(compute_accuracy(test_data_prediction, np.array(ts_lab) == cls))\n",
    "    # print [model.predict(average_vector(dictionary, line[1].lower())) for line in train_data]\n",
    "    return [model.predict_proba(average_vector(dictionary, line[1].lower())) for line in train_data]\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(data, threshold=150):\n",
    "    result_data = []\n",
    "    stop_word_dict = dict()\n",
    "    for line in data:\n",
    "        words = line[1].split()\n",
    "        first = True\n",
    "        for w in words:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            w = w.lower()\n",
    "            if (w in stop_word_dict):\n",
    "                cnt = stop_word_dict[w]\n",
    "                stop_word_dict[w] = cnt + 1\n",
    "            else:\n",
    "                stop_word_dict[w] = 1\n",
    "#     print sorted(stop_word_dict.items(), key=operator.itemgetter(1))\n",
    "    for i, line in enumerate(data):\n",
    "        res = []\n",
    "        first = True\n",
    "        for w in line[1].split():            \n",
    "            if (first or stop_word_dict[w.lower()] < threshold):     \n",
    "                res.append(w)\n",
    "            first = False\n",
    "        result_data.append((line[0],\" \".join(res)))\n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "word_vector_path = \"data/glove.6B.50d.txt\"\n",
    "training_data_path = \"data/train_5500.label\"\n",
    "testing_data_path = \"data/TREC_10.label\"\n",
    "vector_dim = 50\n",
    "word_vector = gensim.models.Word2Vec.load_word2vec_format(word_vector_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with fine grained question classes:\n",
      "Train accuracy 0.523110785033\n",
      "Test accuracy 0.53\n",
      "Accuracy with coarse grained question classes:\n",
      "Train accuracy 0.687820983125\n",
      "Test accuracy 0.722\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(training_data_path)\n",
    "test_data = load_data(testing_data_path)\n",
    "# train_data = remove_stop_words(train_data, 100)\n",
    "# test_data = remove_stop_words(test_data, 100)\n",
    "question_vectors = [average_vector(word_vector, line[1]) for line in train_data]\n",
    "train_labels = [line[0] for line in train_data]\n",
    "test_labels = [line[0] for line in test_data]\n",
    "\n",
    "cfier = linear_model.LogisticRegression(multi_class='multinomial',solver='lbfgs')\n",
    "cfier.fit(question_vectors, train_labels)\n",
    "train_data_prediction = [cfier.predict(average_vector(word_vector, line[1].lower())) for line in train_data]\n",
    "test_data_prediction = [cfier.predict(average_vector(word_vector, line[1].lower())) for line in test_data]\n",
    "\n",
    "print \"Accuracy with fine grained question classes:\"\n",
    "print \"Train accuracy \" + str(compute_accuracy(train_data_prediction, train_labels))\n",
    "print \"Test accuracy \" + str(compute_accuracy(test_data_prediction, test_labels))\n",
    "\n",
    "print (\"Accuracy with coarse grained question classes:\")\n",
    "cfier = linear_model.LogisticRegression(multi_class='multinomial',solver='lbfgs')\n",
    "coarse_test_labels = [line[0].split(\":\")[0] for line in test_data]\n",
    "coarse_train_labels = [line[0].split(\":\")[0] for line in train_data]\n",
    "cfier.fit(question_vectors, coarse_train_labels)\n",
    "train_data_prediction = [cfier.predict(average_vector(word_vector, line[1].lower())) for line in train_data]\n",
    "test_data_prediction = [cfier.predict(average_vector(word_vector, line[1].lower())) for line in test_data]\n",
    "print \"Train accuracy \" + str(compute_accuracy(train_data_prediction, coarse_train_labels))\n",
    "print \"Test accuracy \" + str(compute_accuracy(test_data_prediction, coarse_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544 1670\n",
      "Train accuracy for class LOC: 0.847150259067\n",
      "Test accuracy for class LOC: 0.818\n",
      "2180 2446\n",
      "Train accuracy for class HUM: 0.842201834862\n",
      "Test accuracy for class HUM: 0.898\n",
      "1631 1792\n",
      "Train accuracy for class NUM: 0.82219497241\n",
      "Test accuracy for class NUM: 0.806\n",
      "163 172\n",
      "Train accuracy for class ABBR: 0.815950920245\n",
      "Test accuracy for class ABBR: 0.728\n",
      "2207 2500\n",
      "Train accuracy for class ENTY: 0.750792931581\n",
      "Test accuracy for class ENTY: 0.588\n",
      "2096 2324\n",
      "Train accuracy for class DESC: 0.787213740458\n",
      "Test accuracy for class DESC: 0.782\n"
     ]
    }
   ],
   "source": [
    "#Reduced data set\n",
    "s = set()\n",
    "[s.add(elem) for elem in coarse_train_labels]\n",
    "\n",
    "lab_len = coarse_train_labels.__len__()\n",
    "cls_len = []\n",
    "matrix = []\n",
    "for c in s:\n",
    "    prob = coarse_train_labels.count(c) / float(lab_len)\n",
    "    questions = []\n",
    "    labs = []\n",
    "    for i, q_lab in enumerate(coarse_train_labels):\n",
    "        if (q_lab == c or random.random() < prob):\n",
    "            questions.append(question_vectors[i])\n",
    "            labs.append(q_lab)\n",
    "    print len(questions), 2*coarse_train_labels.count(c)\n",
    "    matrix.append(is_in_class(word_vector, questions, c, labs, coarse_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for class LOC: 0.927549523111\n",
      "Test accuracy for class LOC: 0.93\n",
      "Train accuracy for class HUM: 0.893617021277\n",
      "Test accuracy for class HUM: 0.964\n",
      "Train accuracy for class NUM: 0.920579603815\n",
      "Test accuracy for class NUM: 0.852\n",
      "Train accuracy for class ABBR: 0.98422597212\n",
      "Test accuracy for class ABBR: 0.982\n",
      "Train accuracy for class ENTY: 0.811812179017\n",
      "Test accuracy for class ENTY: 0.838\n",
      "Train accuracy for class DESC: 0.837674247982\n",
      "Test accuracy for class DESC: 0.728\n"
     ]
    }
   ],
   "source": [
    "matrix = []\n",
    "for c in s:\n",
    "    matrix.append(is_in_class(word_vector, question_vectors, c, train_labels, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification into each class separately:\n",
      "Train Accuracy 0.656089508437\n"
     ]
    }
   ],
   "source": [
    "npmatrix = np.array(matrix)\n",
    "\n",
    "m2 = []\n",
    "for i in range(len(matrix[0])):\n",
    "    m2.append(max(npmatrix[:,i,0,1]) == npmatrix[:,i,0,1])\n",
    "\n",
    "l = [i for i in s]    \n",
    "mymap = {}\n",
    "for i,e in enumerate(s):\n",
    "    mymap[e] = i\n",
    "new_lab = np.zeros([len(coarse_train_labels), 6], dtype=bool)\n",
    "for i,l in enumerate(coarse_train_labels):\n",
    "    new_lab[i][mymap[l]] = True\n",
    "\n",
    "cnt = 0\n",
    "for row1, row2 in zip(m2, new_lab):\n",
    "    if False not in (row1 == row2):\n",
    "        cnt += 1\n",
    "\n",
    "print (\"Classification into each class separately:\")\n",
    "print (\"Train Accuracy \" + str(cnt / float(len(m2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ('ENTY:plant', \"What is Australia 's national flower ?\\n\") ['LOC']\n",
      "('HUM:ind', \"What person 's head is on a dime ?\\n\") ['DESC']\n",
      "('NUM:weight', 'What is the average weight of a Yellow Labrador ?\\n') ['ENTY']\n",
      "('NUM:other', 'What is the life expectancy for crickets ?\\n') ['ENTY']\n",
      "('LOC:other', 'What imaginary line is halfway between the North and South Poles ?\\n') ['ENTY']\n",
      "('NUM:speed', 'What is the average speed of the horses at the Kentucky Derby ?\\n') ['ENTY']\n",
      "('NUM:temp', 'What is the temperature at the center of the earth ?\\n') ['LOC']\n",
      "('HUM:gr', 'What is the name of the chocolate company in San Francisco ?\\n') ['LOC']\n",
      "('DESC:desc', 'What is done with worn or outdated flags ?\\n') ['ENTY']\n",
      "('NUM:speed', 'What is the speed hummingbirds fly ?\\n') ['ENTY']\n",
      "('HUM:ind', \"What was W.C. Fields ' real name ?\\n\") ['ENTY']\n",
      "('ENTY:food', 'What do bats eat ?\\n') ['DESC']\n",
      "('ENTY:termeq', 'What do you call a newborn kangaroo ?\\n') ['DESC']\n",
      "('LOC:other', 'What strait separates North America from Asia ?\\n') ['DESC']\n",
      "('NUM:other', 'What is the population of Seattle ?\\n') ['LOC']\n",
      "('ENTY:currency', 'What type of currency is used in Australia ?\\n') ['DESC']\n",
      "('DESC:def', 'What is a prism ?\\n') ['ENTY']\n",
      "('NUM:other', 'What is the population of China ?\\n') ['LOC']\n",
      "('ENTY:event', 'The U.S. Department of Treasury first issued paper currency for the U.S. during which war ?\\n') ['LOC']\n",
      "('DESC:def', 'What is desktop publishing ?\\n') ['ENTY']\n",
      "('NUM:temp', \"What is the temperature of the sun 's surface ?\\n\") ['DESC']\n",
      "('HUM:gr', 'What is the oldest university in the US ?\\n') ['LOC']\n",
      "('NUM:date', 'Mercury , what year was it discovered ?\\n') ['ENTY']\n",
      "('NUM:temp', \"The sun 's core , what is the temperature ?\\n\") ['LOC']\n",
      "('ENTY:animal', 'What is the Ohio state bird ?\\n') ['LOC']\n",
      "('NUM:dist', 'What is the length of the coastline of the state of Alaska ?\\n') ['LOC']\n",
      "('ENTY:plant', \"What is Hawaii 's state flower ?\\n\") ['LOC']\n",
      "('ENTY:substance', 'What mineral helps prevent osteoporosis ?\\n') ['DESC']\n",
      "('NUM:date', 'What was the last year that the Chicago Cubs won the World Series ?\\n') ['ENTY']\n",
      "('NUM:dist', 'What is the diameter of a golf ball ?\\n') ['ENTY']\n",
      "('DESC:def', 'What is an eclipse ?\\n') ['ENTY']\n",
      "('NUM:dist', \"What is the earth 's diameter ?\\n\") ['DESC']\n",
      "('NUM:period', 'What is the gestation period for a cat ?\\n') ['ENTY']\n",
      "('ENTY:other', 'What is the major fault line near Kentucky ?\\n') ['LOC']\n",
      "('HUM:title', 'What position did Willie Davis play in baseball ?\\n') ['DESC']\n",
      "('ENTY:animal', \"What is the name of Roy Roger 's dog ?\\n\") ['HUM']\n",
      "('ENTY:animal', 'What is a baby turkey called ?\\n') ['DESC']\n",
      "('DESC:def', 'What is poliomyelitis ?\\n') ['ENTY']\n",
      "('ENTY:body', 'What is the longest bone in the human body ?\\n') ['DESC']\n",
      "('ENTY:veh', \"What were Christopher Columbus ' three ships ?\\n\") ['NUM']\n",
      "('ENTY:termeq', 'What is another name for vitamin B1 ?\\n') ['DESC']\n",
      "('HUM:gr', 'What baseball team was the first to make numbers part of their uniform ?\\n') ['ENTY']\n",
      "('NUM:date', 'When is hurricane season in the Caribbean ?\\n') ['DESC']\n",
      "('DESC:def', 'What is acetaminophen ?\\n') ['ENTY']\n",
      "('NUM:date', 'When is the summer solstice ?\\n') ['DESC']\n",
      "('ENTY:color', 'What primary colors do you mix to make orange ?\\n') ['DESC']\n",
      "('LOC:other', 'What body of water are the Canary Islands in ?\\n') ['DESC']\n",
      "('ENTY:substance', 'What is natural gas composed of ?\\n') ['DESC']\n",
      "('NUM:dist', 'What is the distance in miles from the earth to the sun ?\\n') ['LOC']\n",
      "('ENTY:product', 'What is the name of the satellite that the Soviet Union sent into space in 1957 ?\\n') ['LOC']\n",
      "('ENTY:animal', \"What are the animals that don 't have backbones called ?\\n\") ['DESC']\n",
      "('NUM:other', 'What is the melting point of copper ?\\n') ['DESC']\n",
      "('DESC:desc', 'What is the effect of acid rain ?\\n') ['ENTY']\n",
      "('LOC:other', 'What New York City structure is also known as the Twin Towers ?\\n') ['ENTY']\n",
      "('ENTY:lang', 'What is the most frequently spoken language in the Netherlands ?\\n') ['LOC']\n",
      "('ENTY:termeq', 'What are the spots on dominoes called ?\\n') ['DESC']\n",
      "('ENTY:other', 'What is the electrical output in Madrid , Spain ?\\n') ['LOC']\n",
      "('NUM:other', 'What is the population of Nigeria ?\\n') ['LOC']\n",
      "('ENTY:other', 'What are the two houses of the Legislative branch ?\\n') ['DESC']\n",
      "('LOC:state', 'What French province is cognac produced in ?\\n') ['HUM']\n",
      "('DESC:def', \"What is Valentine 's Day ?\\n\") ['ENTY']\n",
      "('ENTY:other', 'What is the criterion for being legally blind ?\\n') ['DESC']\n",
      "('LOC:city', 'What are the twin cities ?\\n') ['DESC']\n",
      "('LOC:other', \"What planet is known as the `` red '' planet ?\\n\") ['DESC']\n",
      "('NUM:dist', 'What is the depth of the Nile river ?\\n') ['LOC']\n",
      "('DESC:def', 'What is Mardi Gras ?\\n') ['ENTY']\n",
      "('NUM:money', 'Mexican pesos are worth what in U.S. dollars ?\\n') ['HUM']\n",
      "('ENTY:instru', 'What instrument did Glenn Miller play ?\\n') ['DESC']\n",
      "('ENTY:lang', 'What is the primary language in Iceland ?\\n') ['LOC']\n",
      "('DESC:desc', 'What is the difference between AM radio stations and FM radio stations ?\\n') ['LOC']\n",
      "('DESC:def', 'What is peyote ?\\n') ['ENTY']\n",
      "('ABBR:abb', 'What is the abbreviation for Texas ?\\n') ['ENTY']\n",
      "('HUM:ind', \"What was J.F.K. 's wife 's name ?\\n\") ['ENTY']\n",
      "('ABBR:exp', 'What does I.V. stand for ?\\n') ['DESC']\n",
      "('DESC:def', 'What is the chunnel ?\\n') ['ENTY']\n",
      "('DESC:def', 'What is naproxen ?\\n') ['ENTY']\n",
      "('ENTY:dismed', 'What is foot and mouth disease ?\\n') ['DESC']\n",
      "('ENTY:termeq', 'What do you call a professional map drawer ?\\n') ['DESC']\n",
      "('ENTY:other', 'What does a barometer measure ?\\n') ['DESC']\n",
      "('ABBR:exp', 'What does USPS stand for ?\\n') ['DESC']\n",
      "('NUM:date', 'What date did Neil Armstrong land on the moon ?\\n') ['ENTY']\n",
      "('LOC:other', 'What planet has the strongest magnetic field of all the planets ?\\n') ['DESC']\n",
      "('ENTY:other', 'What are the two types of twins ?\\n') ['DESC']\n",
      "('LOC:other', 'What is the brightest star ?\\n') ['HUM']\n",
      "('ABBR:exp', 'What is TMJ ?\\n') ['DESC']\n",
      "('NUM:date', 'What date was Dwight D. Eisenhower born ?\\n') ['ENTY']\n",
      "('ABBR:exp', 'What does the technical term ISDN mean ?\\n') ['DESC']\n",
      "('ENTY:veh', \"What is the name of William Penn 's ship ?\\n\") ['HUM']\n",
      "('NUM:other', 'What is the melting point of gold ?\\n') ['ENTY']\n",
      "('NUM:perc', 'What is the percentage of water content in the human body ?\\n') ['ENTY']\n",
      "('NUM:perc', 'What is the murder rate in Windsor , Ontario ?\\n') ['LOC']\n",
      "('NUM:other', 'What is the population of Australia ?\\n') ['LOC']\n",
      "('ENTY:dismed', 'Name a stimulant .\\n') ['HUM']\n",
      "('DESC:desc', 'What is the effect of volcanoes on the climate ?\\n') ['LOC']\n",
      "('NUM:date', \"What is the date of Mexico 's independence ?\\n\") ['LOC']\n",
      "('ENTY:plant', 'What is the Illinois state flower ?\\n') ['LOC']\n",
      "('ENTY:animal', \"What is Maryland 's state bird ?\\n\") ['LOC']\n",
      "('NUM:speed', 'What is the speed of light ?\\n') ['ENTY']\n",
      "('NUM:dist', 'What is the width of a football field ?\\n') ['ENTY']\n",
      "('DESC:reason', 'Why in tennis are zero points called love ?\\n') ['LOC']\n",
      "('NUM:dist', 'What is the elevation of St. Louis , MO ?\\n') ['ENTY']\n",
      "('ENTY:color', 'What are the colors of the German flag ?\\n') ['DESC']\n",
      "('LOC:other', 'What soviet seaport is on the Black Sea ?\\n') ['HUM']\n",
      "('NUM:weight', 'What is the atomic weight of silver ?\\n') ['ENTY']\n",
      "('DESC:def', 'What is mad cow disease ?\\n') ['ENTY']\n",
      "('ENTY:food', 'Name a food high in zinc .\\n') ['HUM']\n",
      "('ABBR:exp', 'What does CPR stand for ?\\n') ['DESC']\n",
      "('NUM:other', \"What is the world 's population ?\\n\") ['LOC']\n",
      "('NUM:perc', \"Developing nations comprise what percentage of the world 's population ?\\n\") ['LOC']\n",
      "('HUM:ind', \"What is Shakespeare 's nickname ?\\n\") ['ENTY']\n",
      "('ENTY:substance', 'What is the heaviest naturally occurring element ?\\n') ['DESC']\n",
      "('NUM:date', \"When is Father 's Day ?\\n\") ['DESC']\n",
      "('ABBR:exp', 'What does the acronym NASA stand for ?\\n') ['DESC']\n",
      "('HUM:ind', \"Which U.S.A. president appeared on `` Laugh-In '' ?\\n\") ['ENTY']\n",
      "('ENTY:substance', 'What are cigarettes made of ?\\n') ['DESC']\n",
      "('ABBR:exp', 'What does NASA stand for ?\\n') ['DESC']\n",
      "('ENTY:plant', 'What is the state flower of Michigan ?\\n') ['LOC']\n",
      "('LOC:other', \"What are Canada 's two territories ?\\n\") ['DESC']\n",
      "('DESC:def', 'What is genocide ?\\n') ['ENTY']\n",
      "('ENTY:other', 'What monastery was raided by Vikings in the late eighth century ?\\n') ['LOC']\n",
      "('ENTY:animal', 'What is the name given to the Tiger at Louisiana State University ?\\n') ['LOC']\n",
      "('DESC:def', 'What is vertigo ?\\n') ['ENTY']\n",
      "('NUM:date', 'When is the official first day of summer ?\\n') ['DESC']\n",
      "('ABBR:exp', 'What does the abbreviation SOS mean ?\\n') ['DESC']\n",
      "('ENTY:animal', 'What is the smallest bird in Britain ?\\n') ['LOC']\n",
      "('NUM:date', \"When is St. Patrick 's Day ?\\n\") ['DESC']\n",
      "('ENTY:sport', 'What is the most popular sport in Japan ?\\n') ['LOC']\n",
      "('HUM:gr', 'What did Jesse Jackson organize ?\\n') ['ENTY']\n",
      "('ENTY:animal', \"What is New York 's state bird ?\\n\") ['LOC']\n",
      "('NUM:date', \"What is Susan B. Anthony 's birthday ?\\n\") ['HUM']\n",
      "('ENTY:termeq', 'What do you call a word that is spelled the same backwards and forwards ?\\n') ['DESC']\n",
      "('HUM:gr', 'What chain store is headquartered in Bentonville , Arkansas ?\\n') ['ENTY']\n",
      "('DESC:def', 'What is compounded interest ?\\n') ['NUM']\n",
      "('NUM:other', 'What is the population of Venezuela ?\\n') ['LOC']\n",
      "('ENTY:other', 'What type of polymer is used for bulletproof vests ?\\n') ['DESC']\n",
      "('LOC:city', 'What Canadian city has the largest population ?\\n') ['HUM']\n",
      "('ENTY:other', 'What is the source of natural gas ?\\n') ['LOC']\n",
      "('ENTY:veh', 'In what spacecraft did U.S. astronaut Alan Shepard make his historic 1961 flight ?\\n') ['LOC']\n",
      "('ENTY:currency', 'What is the money they use in Zambia ?\\n') ['LOC']\n",
      "139 500 0.278\n",
      "0.686 0.41\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "what_cnt = 0\n",
    "whatis_cnt = 0\n",
    "for line in test_data:\n",
    "#     print line, cfier.predict(average_vector(word_vector, line[1].lower()))\n",
    "    if (line[0].split(\":\")[0] != cfier.predict(average_vector(word_vector, line[1].lower()))):\n",
    "        print line, cfier.predict(average_vector(word_vector, line[1].lower()))\n",
    "        cnt += 1\n",
    "    if (line[1].split(\" \")[0].lower() == 'what'):\n",
    "        what_cnt += 1\n",
    "        if (line[1].split(\" \")[1].lower() == 'is'):\n",
    "            whatis_cnt += 1\n",
    "print cnt, len(test_data), cnt / float(len(test_data))\n",
    "print what_cnt/float(len(test_data)), whatis_cnt/float(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-e98c5df58ae2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Ahoj jaK se mas neco jen testuju'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
