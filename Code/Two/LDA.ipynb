{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import lda.datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395L, 4258L)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = lda.datasets.load_reuters()\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=500, random_state=1)\n",
    "model.fit(X)\n",
    "topic_word = model.topic_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: government british minister west group letters party former million\n",
      "Topic 1: church first during people political country ceremony visit government\n",
      "Topic 2: elvis king wright fans presley concert life death first\n",
      "Topic 3: yeltsin russian russia president kremlin michael romania orthodox operation\n",
      "Topic 4: pope vatican paul surgery pontiff john hospital trip rome\n",
      "Topic 5: family police miami versace cunanan funeral home church kennedy\n",
      "Topic 6: south simpson born york white north african black wife\n",
      "Topic 7: order church mother successor since election religious head nuns\n",
      "Topic 8: charles prince diana royal queen king parker bowles camilla\n",
      "Topic 9: film france french against actor paris bardot magazine poster\n",
      "Topic 10: germany german war nazi christian letter book scientology jews\n",
      "Topic 11: east prize peace timor quebec belo indonesia nobel award\n",
      "Topic 12: n't told life people church show very public come\n",
      "Topic 13: years world time year last say three later news\n",
      "Topic 14: mother teresa heart charity calcutta missionaries sister order hospital\n",
      "Topic 15: city salonika exhibition buddhist byzantine vietnam swiss capital greek\n",
      "Topic 16: music first people tour including off opera set city\n",
      "Topic 17: church catholic bernardin cardinal bishop death cancer life priest\n",
      "Topic 18: harriman clinton u.s churchill paris president ambassador france american\n",
      "Topic 19: century art million museum city churches works left artists\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 UK: Prince Charles spearheads British royal revolution. LONDON 1996-08-20 (top topic: 8)\n",
      "1 GERMANY: Historic Dresden church rising from WW2 ashes. DRESDEN, Germany 1996-08-21 (top topic: 1)\n",
      "2 INDIA: Mother Teresa's condition said still unstable. CALCUTTA 1996-08-23 (top topic: 14)\n",
      "3 UK: Palace warns British weekly over Charles pictures. LONDON 1996-08-25 (top topic: 8)\n",
      "4 INDIA: Mother Teresa, slightly stronger, blesses nuns. CALCUTTA 1996-08-25 (top topic: 14)\n",
      "5 INDIA: Mother Teresa's condition unchanged, thousands pray. CALCUTTA 1996-08-25 (top topic: 14)\n",
      "6 INDIA: Mother Teresa shows signs of strength, blesses nuns. CALCUTTA 1996-08-26 (top topic: 14)\n",
      "7 INDIA: Mother Teresa's condition improves, many pray. CALCUTTA, India 1996-08-25 (top topic: 14)\n",
      "8 INDIA: Mother Teresa improves, nuns pray for \"miracle\". CALCUTTA 1996-08-26 (top topic: 14)\n",
      "9 UK: Charles under fire over prospect of Queen Camilla. LONDON 1996-08-26 (top topic: 8)\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.34782609e-02,   7.86956522e-02,   4.34782609e-04,\n",
       "         4.82608696e-02,   4.34782609e-04,   4.78260870e-03,\n",
       "         2.65217391e-02,   3.08695652e-02,   4.83043478e-01,\n",
       "         4.78260870e-03,   4.34782609e-04,   2.21739130e-02,\n",
       "         8.30434783e-02,   1.65652174e-01,   4.34782609e-04,\n",
       "         4.34782609e-04,   4.34782609e-04,   1.34782609e-02,\n",
       "         4.34782609e-04,   2.21739130e-02])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print reduce(lambda x, y: x + y, doc_topic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [7, 0, 2, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_table('C:\\Users\\JareD\\Major Project\\EvenSem\\Data\\DataSet_Feature_Extraction_2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = features_df[features_df['Type'] == 'train']['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8666"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, max_features=500, analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'10',\n",
       " u'100',\n",
       " u'11',\n",
       " u'12',\n",
       " u'15',\n",
       " u'18',\n",
       " u'20',\n",
       " u'2000',\n",
       " u'2004',\n",
       " u'2005',\n",
       " u'2006',\n",
       " u'2007',\n",
       " u'2008',\n",
       " u'2009',\n",
       " u'2010',\n",
       " u'2011',\n",
       " u'2012',\n",
       " u'2013',\n",
       " u'20th',\n",
       " u'30',\n",
       " u'50',\n",
       " u'about',\n",
       " u'according',\n",
       " u'across',\n",
       " u'act',\n",
       " u'addition',\n",
       " u'africa',\n",
       " u'after',\n",
       " u'against',\n",
       " u'age',\n",
       " u'air',\n",
       " u'album',\n",
       " u'all',\n",
       " u'along',\n",
       " u'also',\n",
       " u'although',\n",
       " u'america',\n",
       " u'american',\n",
       " u'among',\n",
       " u'an',\n",
       " u'ancient',\n",
       " u'and',\n",
       " u'another',\n",
       " u'any',\n",
       " u'approximately',\n",
       " u'april',\n",
       " u'are',\n",
       " u'area',\n",
       " u'areas',\n",
       " u'army',\n",
       " u'around',\n",
       " u'as',\n",
       " u'asia',\n",
       " u'at',\n",
       " u'august',\n",
       " u'australia',\n",
       " u'available',\n",
       " u'award',\n",
       " u'awards',\n",
       " u'based',\n",
       " u'basis',\n",
       " u'battle',\n",
       " u'bc',\n",
       " u'be',\n",
       " u'became',\n",
       " u'because',\n",
       " u'become',\n",
       " u'been',\n",
       " u'before',\n",
       " u'began',\n",
       " u'being',\n",
       " u'best',\n",
       " u'between',\n",
       " u'bill',\n",
       " u'billion',\n",
       " u'black',\n",
       " u'body',\n",
       " u'book',\n",
       " u'books',\n",
       " u'born',\n",
       " u'both',\n",
       " u'bowl',\n",
       " u'british',\n",
       " u'business',\n",
       " u'but',\n",
       " u'by',\n",
       " u'california',\n",
       " u'called',\n",
       " u'can',\n",
       " u'canada',\n",
       " u'capital',\n",
       " u'card',\n",
       " u'care',\n",
       " u'career',\n",
       " u'cases',\n",
       " u'cause',\n",
       " u'census',\n",
       " u'center',\n",
       " u'central',\n",
       " u'century',\n",
       " u'children',\n",
       " u'china',\n",
       " u'cities',\n",
       " u'city',\n",
       " u'civil',\n",
       " u'college',\n",
       " u'common',\n",
       " u'commonly',\n",
       " u'company',\n",
       " u'computer',\n",
       " u'confederate',\n",
       " u'congress',\n",
       " u'considered',\n",
       " u'constitution',\n",
       " u'control',\n",
       " u'could',\n",
       " u'countries',\n",
       " u'country',\n",
       " u'county',\n",
       " u'court',\n",
       " u'created',\n",
       " u'current',\n",
       " u'currently',\n",
       " u'cut',\n",
       " u'data',\n",
       " u'day',\n",
       " u'death',\n",
       " u'december',\n",
       " u'design',\n",
       " u'developed',\n",
       " u'development',\n",
       " u'did',\n",
       " u'died',\n",
       " u'different',\n",
       " u'do',\n",
       " u'does',\n",
       " u'due',\n",
       " u'during',\n",
       " u'each',\n",
       " u'early',\n",
       " u'earth',\n",
       " u'east',\n",
       " u'economic',\n",
       " u'economy',\n",
       " u'either',\n",
       " u'elected',\n",
       " u'election',\n",
       " u'end',\n",
       " u'energy',\n",
       " u'england',\n",
       " u'english',\n",
       " u'especially',\n",
       " u'established',\n",
       " u'estimated',\n",
       " u'europe',\n",
       " u'european',\n",
       " u'even',\n",
       " u'events',\n",
       " u'example',\n",
       " u'family',\n",
       " u'february',\n",
       " u'federal',\n",
       " u'film',\n",
       " u'financial',\n",
       " u'first',\n",
       " u'five',\n",
       " u'following',\n",
       " u'food',\n",
       " u'football',\n",
       " u'for',\n",
       " u'forces',\n",
       " u'form',\n",
       " u'formed',\n",
       " u'forms',\n",
       " u'found',\n",
       " u'four',\n",
       " u'france',\n",
       " u'free',\n",
       " u'french',\n",
       " u'from',\n",
       " u'game',\n",
       " u'games',\n",
       " u'general',\n",
       " u'generally',\n",
       " u'george',\n",
       " u'germany',\n",
       " u'given',\n",
       " u'global',\n",
       " u'government',\n",
       " u'great',\n",
       " u'group',\n",
       " u'groups',\n",
       " u'had',\n",
       " u'harry',\n",
       " u'has',\n",
       " u'have',\n",
       " u'having',\n",
       " u'he',\n",
       " u'health',\n",
       " u'held',\n",
       " u'her',\n",
       " u'high',\n",
       " u'higher',\n",
       " u'him',\n",
       " u'his',\n",
       " u'history',\n",
       " u'home',\n",
       " u'house',\n",
       " u'however',\n",
       " u'human',\n",
       " u'if',\n",
       " u'ii',\n",
       " u'important',\n",
       " u'in',\n",
       " u'include',\n",
       " u'included',\n",
       " u'including',\n",
       " u'india',\n",
       " u'information',\n",
       " u'international',\n",
       " u'internet',\n",
       " u'into',\n",
       " u'is',\n",
       " u'it',\n",
       " u'its',\n",
       " u'january',\n",
       " u'japan',\n",
       " u'john',\n",
       " u'july',\n",
       " u'june',\n",
       " u'key',\n",
       " u'king',\n",
       " u'kingdom',\n",
       " u'known',\n",
       " u'land',\n",
       " u'language',\n",
       " u'languages',\n",
       " u'large',\n",
       " u'largest',\n",
       " u'last',\n",
       " u'late',\n",
       " u'later',\n",
       " u'latin',\n",
       " u'law',\n",
       " u'leading',\n",
       " u'league',\n",
       " u'least',\n",
       " u'led',\n",
       " u'left',\n",
       " u'less',\n",
       " u'level',\n",
       " u'life',\n",
       " u'like',\n",
       " u'list',\n",
       " u'live',\n",
       " u'located',\n",
       " u'long',\n",
       " u'made',\n",
       " u'main',\n",
       " u'major',\n",
       " u'make',\n",
       " u'making',\n",
       " u'many',\n",
       " u'march',\n",
       " u'market',\n",
       " u'may',\n",
       " u'members',\n",
       " u'metropolitan',\n",
       " u'middle',\n",
       " u'military',\n",
       " u'million',\n",
       " u'model',\n",
       " u'modern',\n",
       " u'money',\n",
       " u'more',\n",
       " u'most',\n",
       " u'movement',\n",
       " u'much',\n",
       " u'music',\n",
       " u'name',\n",
       " u'named',\n",
       " u'national',\n",
       " u'nations',\n",
       " u'natural',\n",
       " u'network',\n",
       " u'new',\n",
       " u'no',\n",
       " u'north',\n",
       " u'northern',\n",
       " u'not',\n",
       " u'november',\n",
       " u'now',\n",
       " u'number',\n",
       " u'numerous',\n",
       " u'ocean',\n",
       " u'october',\n",
       " u'of',\n",
       " u'office',\n",
       " u'official',\n",
       " u'often',\n",
       " u'oil',\n",
       " u'old',\n",
       " u'on',\n",
       " u'one',\n",
       " u'only',\n",
       " u'or',\n",
       " u'order',\n",
       " u'original',\n",
       " u'originally',\n",
       " u'other',\n",
       " u'others',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u'park',\n",
       " u'part',\n",
       " u'particularly',\n",
       " u'party',\n",
       " u'pci',\n",
       " u'people',\n",
       " u'per',\n",
       " u'period',\n",
       " u'person',\n",
       " u'place',\n",
       " u'played',\n",
       " u'player',\n",
       " u'players',\n",
       " u'point',\n",
       " u'political',\n",
       " u'popular',\n",
       " u'population',\n",
       " u'power',\n",
       " u'president',\n",
       " u'presidential',\n",
       " u'primarily',\n",
       " u'primary',\n",
       " u'process',\n",
       " u'produced',\n",
       " u'production',\n",
       " u'professional',\n",
       " u'property',\n",
       " u'provide',\n",
       " u'public',\n",
       " u'published',\n",
       " u'range',\n",
       " u'rate',\n",
       " u'received',\n",
       " u'record',\n",
       " u'recorded',\n",
       " u'records',\n",
       " u'referred',\n",
       " u'refers',\n",
       " u'region',\n",
       " u'released',\n",
       " u'research',\n",
       " u'result',\n",
       " u'right',\n",
       " u'rights',\n",
       " u'river',\n",
       " u'rock',\n",
       " u'role',\n",
       " u'roman',\n",
       " u'sales',\n",
       " u'same',\n",
       " u'san',\n",
       " u'school',\n",
       " u'sea',\n",
       " u'season',\n",
       " u'second',\n",
       " u'see',\n",
       " u'september',\n",
       " u'series',\n",
       " u'served',\n",
       " u'service',\n",
       " u'services',\n",
       " u'set',\n",
       " u'seven',\n",
       " u'several',\n",
       " u'she',\n",
       " u'show',\n",
       " u'side',\n",
       " u'similar',\n",
       " u'since',\n",
       " u'single',\n",
       " u'site',\n",
       " u'six',\n",
       " u'size',\n",
       " u'slave',\n",
       " u'slavery',\n",
       " u'small',\n",
       " u'so',\n",
       " u'social',\n",
       " u'sold',\n",
       " u'some',\n",
       " u'sometimes',\n",
       " u'song',\n",
       " u'south',\n",
       " u'southern',\n",
       " u'soviet',\n",
       " u'space',\n",
       " u'spanish',\n",
       " u'special',\n",
       " u'species',\n",
       " u'standard',\n",
       " u'state',\n",
       " u'states',\n",
       " u'still',\n",
       " u'such',\n",
       " u'super',\n",
       " u'support',\n",
       " u'surface',\n",
       " u'system',\n",
       " u'systems',\n",
       " u'tax',\n",
       " u'team',\n",
       " u'teams',\n",
       " u'television',\n",
       " u'ten',\n",
       " u'term',\n",
       " u'terms',\n",
       " u'territory',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'them',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'third',\n",
       " u'this',\n",
       " u'those',\n",
       " u'though',\n",
       " u'three',\n",
       " u'through',\n",
       " u'throughout',\n",
       " u'time',\n",
       " u'times',\n",
       " u'to',\n",
       " u'today',\n",
       " u'together',\n",
       " u'top',\n",
       " u'total',\n",
       " u'trade',\n",
       " u'two',\n",
       " u'type',\n",
       " u'types',\n",
       " u'typically',\n",
       " u'under',\n",
       " u'union',\n",
       " u'united',\n",
       " u'university',\n",
       " u'until',\n",
       " u'up',\n",
       " u'us',\n",
       " u'use',\n",
       " u'used',\n",
       " u'using',\n",
       " u'usually',\n",
       " u'various',\n",
       " u'version',\n",
       " u'very',\n",
       " u'video',\n",
       " u'virginia',\n",
       " u'vitamin',\n",
       " u'war',\n",
       " u'was',\n",
       " u'washington',\n",
       " u'water',\n",
       " u'way',\n",
       " u'web',\n",
       " u'well',\n",
       " u'were',\n",
       " u'west',\n",
       " u'western',\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'white',\n",
       " u'who',\n",
       " u'widely',\n",
       " u'will',\n",
       " u'windows',\n",
       " u'with',\n",
       " u'within',\n",
       " u'without',\n",
       " u'won',\n",
       " u'word',\n",
       " u'work',\n",
       " u'world',\n",
       " u'worlds',\n",
       " u'worldwide',\n",
       " u'would',\n",
       " u'written',\n",
       " u'year',\n",
       " u'years',\n",
       " u'york']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cosine_similarity(np.array([1, 0, -1]).reshape(1, -1), np.array([-1,-1, 0]).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(vocabulary = vectorizer.get_feature_names(), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 222)\t1\n",
      "  (0, 251)\t1\n",
      "  (0, 440)\t1\n"
     ]
    }
   ],
   "source": [
    "print vectorizer2.fit_transform([\"Hello how is your life today\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-1f11e7f5e6a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: reshape() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "cosine_similarity(Y.reshape(1, -1), X[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(Y, X).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 1.0, 2.0, 3.0, 4.0, 5.0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20L, 500L)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=500, random_state=1)\n",
    "model.fit(X)\n",
    "topic_word = model.topic_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTY': {0: [0.61029946887328501, 0.026185157045858505, 0.73979018680290409, 0.049705206626920705, 0.070038568584980893, 0.17382139680965189, 0.14284657149944902, 0.054937669692581827, 0.10256715773403591, 0.089588929692460559], 1: [0.00081762778396759096, 0.0010788622582196521, 0.99996491560047507, 0.0031750627711143145, 0.0021213744446699216, 0.0025921675936261116, 0.004481918321645518, 0.0013021981864938447, 0.003015860050143428, 0.004019427745390213]}, 'LOC': {0: [0.1418234217053389, 0.082711314201614344, 0.098667357556026722, 0.14043445914340572, 0.32038014417898952, 0.089993130055563156, 0.069603369066054896, 0.26226197569440923, 0.86931209359330885, 0.0592973996683838], 1: []}, 'NUM': {0: [0.021681866217045032, 0.061814156636969216, 0.03097545001188334, 0.10161991882504835, 0.1719079898574675, 0.94282443542366123, 0.060725673775915985, 0.16939172879569628, 0.15813642991129767, 0.092543241770283655], 1: [0.01954633669583912, 0.050969099891904734, 0.029542782151223697, 0.020251854260637366, 0.055132329002270629, 0.92353716223318583, 0.0098422575196347988, 0.33722981930660489, 0.0085759172755588958, 0.16087991438662388]}, 'DESC': {0: [0.0095882059401316017, 0.0081856685613564267, 0.013728431870745203, 0.82316161188803405, 0.49131319735447132, 0.25406506715618632, 0.061706808318304524, 0.062125251668579566, 0.013678696353282215, 0.090914530983360886], 1: [0.0038950971411621711, 0.11448710020303965, 0.0088386695528620829, 0.83981389953850083, 0.52324146817210171, 0.086595201815607842, 0.0032467775976845951, 0.0033793272480686764, 0.0035088019557347068, 0.014049486355980179]}, 'HUM': {0: [0.21170880909843368, 0.085510985203643239, 0.23144948293278048, 0.44733175388920765, 0.14136355535850623, 0.69864821683239919, 0.37388496036803248, 0.064283078972795601, 0.19983309315461378, 0.047372584414639901], 1: [0.17036059521863486, 0.010195460139561156, 0.56215423310107071, 0.72089680211693485, 0.18071221468341903, 0.19172215313032884, 0.049741255243874252, 0.13241236667635753, 0.15666187383927796, 0.1455997270594945]}}\n",
      "[0.010048952307564744, 0.037733238590551291, 0.7867894687001632, 0.02103968230353501, 0.61433747935758465, 0.014321644733419505, 0.012177686918147374, 0.020618976837017685, 0.012730430587975079, 0.025371929278874428]\n",
      "[[ 0.35218008]] [[ 0.33850963]]\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import lda.datasets\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "#Latent Dirichlet Allocation\n",
    "\n",
    "def buildLDA(features_df):\n",
    "    #Vectorize the sentences\n",
    "    sentences = features_df['Sentence']\n",
    "    vectorizer = CountVectorizer(max_features=5000, analyzer='word')\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    \n",
    "    #Cluster the vectors\n",
    "    model = lda.LDA(n_topics = 10, n_iter = 200, random_state = 1)\n",
    "    model.fit(X)\n",
    "    topic_word = model.topic_word_\n",
    "    doc_topic = model.doc_topic_\n",
    "    return model, vocabulary\n",
    "\n",
    "def getTopicVector(sentence, model, vectorizer):\n",
    "    #model, vocabulary = buildLDA()\n",
    "    #vectorizer = CountVectorizer(analyzer='word', vocabulary = vocabulary)\n",
    "    #sentence = sentence.decode('utf-8','ignore').encode(\"utf-8\")\n",
    "    #print sentence\n",
    "    X = vectorizer.fit_transform([sentence])\n",
    "    topic_vector = model.transform(X)\n",
    "    return topic_vector\n",
    "\n",
    "#Getting average vectors\n",
    "def getAverageVectors(topic_vectors):\n",
    "    average_topic_vectors = {}\n",
    "    for key, value in topic_vectors.iteritems():\n",
    "        average_topic_vectors[key] = {}\n",
    "        average_topic_vectors[key][0] = np.squeeze(np.asarray(np.matrix(topic_vectors[key][0]).sum(axis = 0)))/len(topic_vectors[key][0])\n",
    "        average_topic_vectors[key][1] = np.squeeze(np.asarray(np.matrix(topic_vectors[key][1]).sum(axis = 0)))/len(topic_vectors[key][1])\n",
    "        average_topic_vectors[key][0] = normalizeVector(average_topic_vectors[key][0])\n",
    "        average_topic_vectors[key][1] = normalizeVector(average_topic_vectors[key][1])\n",
    "        \n",
    "    return average_topic_vectors\n",
    "\n",
    "def normalizeVector(vector):\n",
    "    vector_magnitude = math.sqrt(sum([x * x for x in vector]))\n",
    "    vector = [x/vector_magnitude for x in vector]\n",
    "    return vector\n",
    "\n",
    "def getLDAFeature():\n",
    "    #Training the model and soft clustering sentences\n",
    "    features_df = pd.read_table('C:\\Users\\JareD\\Major Project\\EvenSem\\Data\\DataSet_Feature_Extraction_2.tsv')\n",
    "    features_df_train = features_df[features_df['Type'] == 'train']\n",
    "    model, vocabulary = buildLDA(features_df_train)\n",
    "    vectorizer = CountVectorizer(analyzer='word', vocabulary = vocabulary)\n",
    "    #x = 1\n",
    "    topic_vectors = {}\n",
    "    \n",
    "    for question_type in features_df_train['QuestionType'].unique():\n",
    "        topic_vectors[question_type] = {1 : [], 0 : []}\n",
    "        df = features_df_train[features_df_train['QuestionType'] == question_type]\n",
    "        df = df.head(10)\n",
    "        for index, row in df.iterrows():\n",
    "            topic_vector = getTopicVector(row['Sentence'], model, vectorizer)\n",
    "            if row['Label'] == 1:\n",
    "                topic_vectors[question_type][1].append(topic_vector[0])\n",
    "            else:\n",
    "                topic_vectors[question_type][0].append(topic_vector[0])\n",
    "    \n",
    "    \n",
    "    #x = 2\n",
    "    average_topic_vectors = getAverageVectors(topic_vectors)\n",
    "    print average_topic_vectors\n",
    "    #x = 3\n",
    "    #Getting cosine similarity\n",
    "    sentence = \"the world is not big enough for two super villains\"\n",
    "    topic_vector = getTopicVector(sentence, model, vectorizer)\n",
    "    topic_vector = normalizeVector(topic_vector[0])\n",
    "    print topic_vector\n",
    "    positive_similarity = cosine_similarity(average_topic_vectors['DESC'][1], topic_vector)\n",
    "    negative_similarity = cosine_similarity(average_topic_vectors['DESC'][0], topic_vector)\n",
    "    #print positive_similarity, negative_similarity\n",
    "    #x = 4\n",
    "    return positive_similarity, negative_similarity\n",
    "    \n",
    "    \n",
    "\n",
    "positive_similarity, negative_similarity = getLDAFeature()       \n",
    "print positive_similarity, negative_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.55516948961\n"
     ]
    }
   ],
   "source": [
    "vector = topic_vector[0]\n",
    "print sum(vector)\n",
    "vector_magnitude = math.sqrt(sum([x * x for x in vector]))\n",
    "vector = [x/vector_magnitude for x in vector]\n",
    "print sum(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting average vectors\n",
    "def getAverageVectors():\n",
    "    average_topic_vectors = {}\n",
    "    for key, value in topic_vectors.iteritems():\n",
    "        average_topic_vectors[key][0] = np.squeeze(np.asarray(np.matrix(topic_vectors[key][0]).sum(axis = 0)))/len(topic_vectors[key][0])\n",
    "        average_topic_vectors[key][1] = np.squeeze(np.asarray(np.matrix(topic_vectors[key][1]).sum(axis = 0)))/len(topic_vectors[key][1])\n",
    "    return average_topic_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTY\n",
      "1064\n",
      "154\n",
      "LOC\n",
      "1201\n",
      "173\n",
      "HUM\n",
      "1312\n",
      "169\n",
      "NUM\n",
      "2010\n",
      "208\n",
      "DESC\n",
      "2040\n",
      "335\n",
      "8666\n",
      "8666\n"
     ]
    }
   ],
   "source": [
    "#verification (ignore)\n",
    "sums = 0\n",
    "for key, value in topic_vectors.iteritems():\n",
    "    print key\n",
    "    print len(value[0])\n",
    "    print len(value[1])\n",
    "    sums = sums + len(value[0]) + len(value[1])\n",
    "\n",
    "print sums\n",
    "print len(features_df[features_df['Type'] == 'train'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08126983,  0.03974846,  0.06698567,  0.17432506,  0.17846414,\n",
       "        0.19014801,  0.04245724,  0.07072159,  0.09212471,  0.06081411])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(np.asarray(np.matrix(topic_vectors['DESC'][0]).sum(axis = 0)))/len(topic_vectors['DESC'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.19617077e-07   6.19617077e-07   6.19617077e-07 ...,   6.19617077e-07\n",
      "    6.19617077e-07   6.19617077e-07]\n",
      " [  6.43128175e-07   6.43128175e-07   6.43128175e-07 ...,   6.43128175e-07\n",
      "    6.43128175e-07   6.43128175e-07]\n",
      " [  9.48071638e-04   1.80090969e-03   4.73798920e-07 ...,   4.73798920e-07\n",
      "    4.73798920e-07   4.73798920e-07]\n",
      " ..., \n",
      " [  3.36752271e-03   3.13263389e-03   7.83745694e-04 ...,   7.82962731e-07\n",
      "    7.82962731e-07   4.70560601e-04]\n",
      " [  7.57059581e-07   7.57059581e-07   7.57059581e-07 ...,   7.57059581e-07\n",
      "    7.57059581e-07   2.27874934e-04]\n",
      " [  6.45869664e-07   6.45869664e-07   6.45869664e-07 ...,   6.45869664e-07\n",
      "    3.23580701e-04   6.45869664e-07]]\n"
     ]
    }
   ],
   "source": [
    "sentences = features_df[features_df['Type'] == 'train']['Sentence']\n",
    "vectorizer = CountVectorizer(max_features=5000, analyzer='word')\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "model = lda.LDA(n_topics = 10, n_iter = 500, random_state = 1)\n",
    "model.fit(X)\n",
    "topic_word = model.topic_word_\n",
    "doc_topic = model.doc_topic_\n",
    "print topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer(analyzer='word', vocabulary = ['green', 'red', 'blue'])\n",
    "print v.fit_transform(['hello green red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20 * [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[3]][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "average_topic_vectors = pickle.load(open(\"C:\\Users\\JareD\\Major Project\\EvenSem\\Models\\LDA\\Average_Topic_Vectors_2.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC\n",
      "0\n",
      "[0.12727258866973465, 0.23355766765640823, 0.23625923624920767, 0.26466063986303645, 0.16917651146857721, 0.18573996810491286, 0.17407322238388548, 0.086128609442843312, 0.21156725514318908, 0.13625795439004004, 0.21932634475646839, 0.15152792316706304, 0.16958467834473726, 0.56564495663469283, 0.23403780981410893, 0.19158550942426197, 0.16585997595278731, 0.25922671196163943, 0.13040242003833497, 0.12748641359254029]\n",
      "1\n",
      "[0.1762598207871888, 0.094811090047981675, 0.082356131389920234, 0.12642704728646509, 0.040884166415926466, 0.089163481830290978, 0.071668551248603357, 0.030835361398159355, 0.39830715939718342, 0.063151422022759834, 0.086683242556200565, 0.08088204282760772, 0.12778147958954192, 0.81416773137286924, 0.17396263999076805, 0.07622233660794013, 0.096872950178936768, 0.11215419973027423, 0.067158329846123535, 0.055066821057391593]\n",
      "HUM\n",
      "0\n",
      "[0.32055426067216602, 0.23989968536179862, 0.18597727872946082, 0.19623103939668424, 0.22639537898191017, 0.17865134950147915, 0.18172728135890087, 0.13291198711024713, 0.1682554484983555, 0.15102301144239846, 0.14506532352025678, 0.25282305735992683, 0.17179412984843287, 0.15725412694123597, 0.15849509883835661, 0.33522521201454586, 0.20801148570423114, 0.18346912899459777, 0.28922615818062825, 0.37991562150632274]\n",
      "1\n",
      "[0.39811213407537616, 0.11264738850301181, 0.092352074511199672, 0.14928702555596154, 0.1110348683068285, 0.14840770652930124, 0.15321142705894378, 0.16026925339832435, 0.18427681279493832, 0.15863574341486653, 0.090119316332101351, 0.15693681498762788, 0.1657292045004585, 0.14433481538698753, 0.077471446423404464, 0.36076864011950627, 0.20861170592179037, 0.46482716054983975, 0.22333833964614144, 0.35874310982746083]\n",
      "NUM\n",
      "0\n",
      "[0.23019734892190014, 0.19036230090693956, 0.23392572598921357, 0.34167218224512735, 0.22778178812742172, 0.20784236029377781, 0.17483296965637612, 0.15716944782094963, 0.18331042121880847, 0.18481989871592752, 0.1809688270218702, 0.2202937754221253, 0.17730149040215085, 0.18433805155610236, 0.22344372883427632, 0.27853127269956285, 0.22460698486505393, 0.20516400776337851, 0.30493343256009831, 0.24793187827847066]\n",
      "1\n",
      "[0.6378566646510706, 0.31974842757390692, 0.16749389693073091, 0.26674455102415573, 0.064400309969847219, 0.18510152946509842, 0.15032188167668856, 0.066365375903779486, 0.19332819778643284, 0.094754269591283038, 0.10886965574249644, 0.17523415722089186, 0.17711574467148566, 0.15979091638052764, 0.16462775857074144, 0.26606038101672796, 0.12725178143562171, 0.15730101187129292, 0.11436838709681481, 0.16899074885305015]\n",
      "ABBR\n",
      "0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "ENTY\n",
      "0\n",
      "[0.16723942017884955, 0.14491030000778032, 0.20805199570874713, 0.37078386211380243, 0.20527706929320497, 0.19362797386626476, 0.32465184202764785, 0.21298392314405765, 0.15558966018320269, 0.27278010668546238, 0.26002650243473319, 0.15607128966614636, 0.29236099089821027, 0.10800443956331562, 0.26853581228062801, 0.18817950878414158, 0.18898222184660307, 0.20075079783448671, 0.20200689552151008, 0.16673395958669809]\n",
      "1\n",
      "[0.25739849607589638, 0.10761140927007448, 0.1181805268271492, 0.26471631802585799, 0.093659777789915813, 0.11967264077267993, 0.34428386255595211, 0.22170247010233701, 0.17937216003159959, 0.21555029529450997, 0.13521485922934942, 0.15811419036580324, 0.46384046980971971, 0.10467639855813875, 0.29145089710057404, 0.14609267569858322, 0.25799054324839243, 0.3115501055800981, 0.12911038707570666, 0.11288937867960037]\n",
      "DESC\n",
      "0\n",
      "[0.12787126201107335, 0.14108502409078419, 0.23734313228977325, 0.10857015536335235, 0.21481634021790752, 0.19495606831735174, 0.36064246288296065, 0.3435423330745232, 0.12934763441199751, 0.32604485305639669, 0.3018603203464395, 0.12051893742812411, 0.29310587555234069, 0.12750587535229196, 0.35268825508763663, 0.11329746483202947, 0.15111638098194277, 0.19645040341629158, 0.14898056701264945, 0.10810960962190055]\n",
      "1\n",
      "[0.12883597867643623, 0.059508721746117529, 0.14991258507672711, 0.060255821384195614, 0.16974393839207941, 0.23218190473755293, 0.34791326629886277, 0.35176642384531415, 0.11327141253825186, 0.32641152480344637, 0.15481042331933118, 0.087738152222339966, 0.391081802041575, 0.10161361252890286, 0.43595179282046687, 0.073253108447699514, 0.23665448610414624, 0.21812733603036077, 0.081170402437097972, 0.079673136904780797]\n"
     ]
    }
   ],
   "source": [
    "for key, value in average_topic_vectors.iteritems():\n",
    "    print key\n",
    "    print \"0\"\n",
    "    print value[0]\n",
    "    print \"1\"\n",
    "    print value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
